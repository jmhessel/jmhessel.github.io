<html, lang="en">
<head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73020809-1', 'auto');
  ga('send', 'pageview');

</script>
<link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>
<style>
      body {
        font-family: 'Roboto', serif;
        font-size: 20px;
      }
    </style>
</head>
<body>
<title>Jack Hessel's Homepage</title>
<h2>Jack Hessel</h2>
<p>contact: jmhessel@gmail.com</p>
<p><a href="cv.pdf">CV</a> (as of 12/2023).</p>
<p>I am on <a href="https://github.com/jmhessel/">github,</a> <a href="https://www.twitter.com/jmhessel">twitter,</a> and <a href="https://scholar.google.com/citations?user=SxQQ1msAAAAJ">google scholar</a>.</p>
<p>
I am a machine learning researcher at <a href="https://samaya.ai">Samaya AI</a>. Previously, I was a postdoc/research scientist at AI2, and before that, I earned a PhD in Computer Science at Cornell University. If you're looking for a bio for a talk introduction, <a href="talk_bio.txt">here's one.</a> If you're looking for me in person, I look something like this (facial hair subject to cha<a href="me.jpg">n</a><a href="beard.jpg">g</a><a href="me2.jpg">e</a>)<a href="extras.html">:</a> </p>
<p>
<img src="main_photo.jpg" alt="Me" style="width:200px">
</p>

<h3>Selected projects:</h3>
<ul>
  <li>
    <a href="https://arxiv.org/abs/2209.06293">Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest</a><br>
    <b>Jack Hessel</b>, Ana Marasoviƒá, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, Yejin Choi <br>
    ACL 2023 <br>
    üèÜ Best paper award üèÜ (one of three selections from 3872 submissions)</i> <br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/pdf/2010.06572.pdf">Does my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think!</a><br>
    <b>Jack Hessel</b> and Lillian Lee <br>
    EMNLP 2020<br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2106.02636">MERLOT: Multimodal Neural Script Knowledge Models</a><br>
    Rowan  Zellers*, Ximing  Lu*, <b>Jack Hessel*</b>, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.<br>
    NeurIPS 2021 (oral)<br>
  </li>
</ul>



<h3>Some recent preprints:</h3>
<ul>
  <li>
    <a href="https://arxiv.org/abs/2402.15610">Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning</a><br>
    Tejas Srinivasan, <b>Jack Hessel,</b> Tanmay Gupta, Bill Yuchen Lin, Yejin Choi, Jesse Thomason, Khyathi Raghavi Chandu<br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2402.03284">Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models</a><br>
    Anthony Sicilia, Hyunwoo Kim, Khyathi Raghavi Chandu, Malihe Alikhani, <b>Jack Hessel</b><br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2402.09052">L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects</a><br>
    Yutaro Yamada, Khyathi Chandu, Yuchen Lin, <b>Jack Hessel,</b> Ilker Yildirim, Yejin Choi<br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2402.00838">OLMo: Accelerating the Science of Language Models</a><br>
    Dirk Groeneveld, Iz Beltagy, + 41 more folks, including me :-) <br>
    Try <a href="https://huggingface.co/allenai/OLMo-7B">OLMO-7B</a>, one of the biggest+most open LLMs in the world!
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2311.08469">FunQA: Towards Surprising Video Comprehension</a><br>
    Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, <b>Jack Hessel,</b> Jingkang Yang, Ziwei Liu <br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2310.11564">Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging</a><br>
    Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, <b>Jack Hessel,</b> Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu<br>
    <a href="https://github.com/joeljang/RLPHF">Mix your own soup!</a>
  </li>
  <p>
</ul>


<h3>Publications (in reverse chronological order)</h3>
<ul>
  <p>
    <li>
      <a href="https://arxiv.org/abs/2311.08469">UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations</a><br>
      Wenting Zhao, Justin T Chiu, Jena D. Hwang, Faeze Brahman, <b>Jack Hessel,</b> Sanjiban Choudhury, Yejin Choi, Xiang Lorraine Li, Alane Suhr<br>
      NAACL 2024 <br>
      code/data coming soon!
    </li>
  <p>
    <li>
      <a href="https://openreview.net/pdf?id=Bl8u7ZRlbM">(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild</a><br>
      Wenting Zhao, Xiang Ren, <b>Jack Hessel,</b> Claire Cardie, Yejin Choi, Yuntian Deng<br>
      ICLR 2024 <br>
      <a href="https://wildchat.allen.ai/">code/models</a>
    </li>
  <p>
    <li>
      <a href="https://arxiv.org/abs/2308.01390">Tailoring Self-Rationalizers with Multi-Reward Distillation</a><br>
      Sahana Ramnath, Brihi Joshi, Skyler Hallinan, Ximing Lu, Liunian Harold Li, Aaron Chan, <b>Jack Hessel,</b> Yejin Choi, Xiang Ren<br>
      ICLR 2024 <br>
      <a href="https://github.com/INK-USC/RationaleMultiRewardDistillation">code/models</a>
    </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2308.01390">OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</a><br>
    Anas Awadalla*, Irena Gao*, Josh Gardner, <b>Jack Hessel</b>, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt <br>
    <a href="https://github.com/mlfoundations/open_flamingo">code/models</a>, <a href="https://laion.ai/blog/open-flamingo/">blog post</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2212.10465">SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization</a><br>
    Hyunwoo Kim, <b>Jack Hessel,</b> Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, Yejin Choi <br>
    EMNLP 2023 <br>
    <a href="https://huggingface.co/datasets/allenai/soda">SODA dataset;</a> <a href="https://huggingface.co/allenai/cosmo-xl">talk to COSMO here!</a> <br>
    üèÜ Outstanding paper award üèÜ (one of thirty selections from 4909 submissions)<br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2305.14897">Text encoders are performance bottlenecks in contrastive vision-language models</a><br>
    Amita Kamath, <b>Jack Hessel,</b> Kai-Wei Chang <br>
    EMNLP 2023 <br>
    <a href="https://github.com/amitakamath/vl_text_encoders_are_bottlenecks">code/data</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2310.19785">What's "up" with vision-language models? Investigating their struggle to understand spatial relations.</a> <br>
    Amita Kamath, <b>Jack Hessel,</b> Kai-Wei Chang <br>
    EMNLP 2023 <br>
    <a href="https://github.com/amitakamath/whatsup_vlms">code/data</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2310.10418">Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms</a> <br>
    Seungju Han, Junhyeok Kim, <b>Jack Hessel,</b> Liwei Jiang, Jiwan Chung, Yejin Son, Yejin Choi, Youngjae Yu <br>
    EMNLP 2023 <br>
    <a href="https://github.com/wade3han/normlens">code/data</a>
  </li>
  <p> 
  <li>
    <a href="https://arxiv.org/abs/2312.05979">NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation</a> <br>
    Peter West, Ronan Le Bras, Taylor Sorensen, Bill Yuchen Lin, Liwei Jiang, Ximing Lu, Khyathi Chandu, <b>Jack Hessel,</b> Ashutosh Baheti, Chandra Bhagavatula, Yejin Choi <br>
    Findings of EMNLP 2023 <br>
    code/data/models forthcoming!
  </li>
  <p>
  
  <li>
    <a href="https://arxiv.org/abs/2312.04837">Localized Symbolic Knowledge Distillation for Visual Commonsense Models</a><br>
    Jae Sung Park, <b>Jack Hessel</b>, Khyathi Chandu, Paul Pu Liang, Ximing Lu, Qiuyuan Huang, Peter West, Jianfeng Gao, Ali Farhadi, Yejin Choi <br>
    NeurIPS 2023 <br>
    <a href="https://github.com/jamespark3922/lskd">code coming soon!</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2304.06939">Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text</a><br>
    Wanrong Zhu*, <b>Jack Hessel*</b>, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, Yejin Choi <br>
    NeurIPS D+B 2023 <br>
    <a href="https://github.com/allenai/mmc4">mmc4 is now available!</a> <a href="https://www.geekwire.com/2023/ai2-researchers-release-new-multimodal-approach-to-boost-ai-capabilities-using-images-and-audio/">press: geekwire</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2308.06595">VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use</a><br>
    Yonatan Bitton*, Hritik Bansal*, <b>Jack Hessel*</b>, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schimdt <br>
    NeurIPS D+B 2023 <br>
    <a href="https://visit-bench.github.io/">data/leaderboard</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2308.01390">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</a><br>
    Yizhong Wang, Hamish Ivison, Pradeep Dasigi, <b>Jack Hessel</b>, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, Hannaneh Hajishirzi <br>
    NeurIPS D+B 2023 <br>
    <a href="https://github.com/allenai/open-instruct">T√ºlu 65B avilable now!</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2303.09713">CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos</a><br>
    Seungju Han, <b>Jack Hessel,</b> Nouha Dziri, Yejin Choi, Youngjae Yu<br>
    ICCV 2023 <br>
    <a href="https://github.com/wade3han/champagne">code/data/models</a>
  </li>  
  <p>
  <li>
    <a href="https://arxiv.org/abs/2303.07274">Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images</a><br>
    Nitzan Bitton-Guetta*, Yonatan Bitton*, <b>Jack Hessel,</b> Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, Roy Schwartz <br>
    ICCV 2023 <br>
    <i>WHOOPS Dataset available <a href="https://whoops-benchmark.github.io/">here</a></i>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2209.06293">Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest</a><br>
    <b>Jack Hessel</b>, Ana Marasoviƒá, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, Yejin Choi <br>
    ACL 2023 <br>
    üèÜ Best paper award üèÜ (one of three selections from 3872 submissions)</i> <br>
    <i>The New Yorker dataset/leaderboard/code at <a href="http://capcon.dev">capcon.dev</a><br>
  Press: <a href="https://www.theatlantic.com/sponsored/google-2023/dialogues/3841/">The Atlantic</a>; <a href="https://www.nytimes.com/interactive/2022/12/26/magazine/yejin-choi-interview.html">New York Times, in an interview with Yejin</a>; <a href="https://neurosciencenews.com/ai-humor-23736/">Neuroscience News</a>; <a href="https://becauselanguage.com/79-a-i-hype-hosedown/">Because Language podcast</a>; <a href="https://news.cornell.edu/stories/2023/07/thats-funny-ai-models-dont-get-joke">Cornell Chronicle</a>; <a href="https://www.psychologytoday.com/us/blog/the-future-brain/202308/do-ai-models-like-gpt-really-get-the-joke">Psychology Today</a>; <a href="https://www.youtube.com/watch?v=fo6eSL7ntmE">video</a> </i>
  </li>

  <p>
    <li>
      <a href="https://aclanthology.org/2023.acl-long.150/">Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step</a><br>
      Liunian Harold Li, <b>Jack Hessel,</b> Youngjae Yu, Xiang Ren, Kai-Wei Chang and Yejin Choi<br>
      ACL 2023<br>
    </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2205.12630">Multimodal Knowledge Alignment with Reinforcement Learning</a></br>
    Youngjae Yu, Jiwan Chung, Heeseung Yun, <b>Jack Hessel</b>, JaeSung Park, Ximing Lu, Prithviraj Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, Yejin Choi <br>
    CVPR 2023<br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2210.01241">Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</a><br>
    Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant√© Brantley, <b>Jack Hessel</b>, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.<br>
    ICLR 2023 (spotlight) ; also appeared at <a href="https://internlp.github.io/">InterNLP @ NeurIPS 2022.</a> <br>
    <i>Check out the <a href="https://github.com/allenai/RL4LMs">RL4LMs library!</a></i>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2205.13636">Quark: Controllable Text Generation with Reinforced Unlearning</a></br>
    Ximing Lu, Sean Welleck*, <b>Jack Hessel*</b>, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, Yejin Choi. <br>
    NeurIPS 2022 (oral) <br>
    <a href="https://github.com/GXimingLu/Quark">code, models, etc.</a>
  </li>
  <p>    
  <li>
    <a href="https://arxiv.org/abs/2202.04800">The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning.</a><br>
    <b>Jack Hessel*</b>, Jena D Hwang*, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko,
and Yejin Choi. <br>
    ECCV 2022 (oral) <br>
    <a href="https://github.com/allenai/sherlock">dataset/code/leaderboard</a>; <a href="https://venturebeat.com/2022/02/23/this-new-dataset-shows-that-ai-still-lacks-commonsense-reasoning/"> press: venturebeat </a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2112.08674">Reframing Human-AI Collaboration for Generating Free-Text Explanations.</a><br>
    Sarah Wiegreffe, <b>Jack Hessel</b>, Swabha Swayamdipta, Mark Riedl, and Yejin Choi.<br>
    NAACL 2022<br>
    <a href="https://github.com/allenai/few_shot_explanations">code</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2112.08995">Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer.</a><br>
    Yanpeng Zhao, <b>Jack Hessel</b>, Youngjae Yu, Ximing Lu, Rowan Zellers, and Yejin Choi.<br>
    NAACL 2022<br>
    <a href="https://github.com/zhaoyanpeng/vipant">code</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2110.07178">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</a><br>
    Peter West, Chandra Bhagavatula, <b>Jack Hessel</b>, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi<br>
    NAACL 2022<br>
    <a href="https://github.com/peterwestai2/symbolic-knowledge-distillation">code</a>, <a href="https://www.youtube.com/watch?v=kP-dXK9JEhY">explainer video</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2201.02639">MERLOT Reserve: Neural Script Knowledge through Sound, Language, and Vision</a><br>
    Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao,
    Mohammadreza Salehi, Aditya Kusupati, <b>Jack Hessel</b>, Ali Farhadi, Yejin Choi <br>
    CVPR 2022 (oral)<br>
    <a href="https://github.com/rowanz/merlot_reserve">code</a>, <a href="https://rowanzellers.com/merlotreserve/">project page</a>, <a href="https://venturebeat.com/2022/03/22/companies-are-commercializing-multimodal-ai-models-to-analyze-videos-and-more/">Press: Venturebeat</a>, <a href="https://www.geekwire.com/2022/new-ai-model-shows-how-machines-can-learn-from-vision-language-and-sound-together/">Press: GeekWire, with interview from Rowan!</a> <br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2106.02636">MERLOT: Multimodal Neural Script Knowledge Models</a><br>
    Rowan  Zellers*, Ximing  Lu*, <b>Jack Hessel*</b>, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.<br>
    NeurIPS 2021 (oral)<br>
    <a href="https://github.com/rowanz/merlot">code</a>, <a href="https://rowanzellers.com/merlot/">project page</a>, <a href="https://read.deeplearning.ai/the-batch/issue-116/">Press: The Batch</a >, <a href="https://venturebeat.com/2021/06/11/this-ai-system-learned-to-understand-videos-by-watching-youtube/">Press: Venturebeat</a ><br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/2104.08718">CLIPScore: A Reference-free Evaluation Metric for Image Captioning</a><br>
    <b>Jack Hessel</b>, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi<br>
    EMNLP 2021<br>
    <a href="https://github.com/jmhessel/clipscore">code</a>, <a href="https://youtu.be/2jVuyxnfmXA">talk</a> <br>
  </li>
  <p>
  <li>
    <a href="https://aclanthology.org/2021.acl-short.27/">How effective is BERT without word ordering? Implications for language understanding and data privacy.</a><br>
    <b>Jack Hessel</b> and Alexandra Schofield <br>
    ACL 2021<br>
    <a href="https://aclanthology.org/attachments/2021.acl-short.27.OptionalSupplementaryMaterial.zip">code</a>, <a href="https://youtu.be/U-FtJA-KdPk"> talk </a> <br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/pdf/2010.06572.pdf">Does my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think!</a><br>
    <b>Jack Hessel</b> and Lillian Lee <br>
    EMNLP 2020<br>
    <a href="https://github.com/jmhessel/EMAP_EMNLP2020">code</a>, <a href="https://slideslive.com/38938964/does-my-multimodal-model-learn-crossmodal-interactions-its-harder-to-tell-than-you-might-think">talk</a> <br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/pdf/2004.14338.pdf">Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube</a><br>
    <b>Jack Hessel</b>, Zhenhai Zhu, Bo Pang, and Radu Soricut <br>
    EMNLP 2020<br>
    <a href="https://github.com/google-research-datasets/i3-video">data</a>, <a href="https://slideslive.com/38938709/beyond-instructional-videos-probing-for-more-diverse-visualtextual-grounding-on-youtube">talk</a> <br>
  </li>
  <p>
    <li>
      <a href="https://arxiv.org/pdf/2010.16363.pdf">Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents<a/><br>
      Gregory Yauney, <b>Jack Hessel</b>, and David Mimno <br>
      EMNLP 2020 <br>
      <a href="https://github.com/gyauney/domain-specific-lexical-grounding/">code</a><br>
    </li>
  <p>
  <li>
    <a href="files/2020/phd_thesis.pdf">Learning from Multimodal Web Data</a><br>
    PhD Thesis<br>
    Cornell University 2020<br>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/1904.07826">Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents</a><br>
    <b>Jack Hessel</b>, Lillian Lee, and David Mimno<br>
    EMNLP 2019<br>
    <a href="https://github.com/jmhessel/multi-retrieval">code/data</a>, <a href="files/2019/EMNLP2019_poster_final.pdf">poster</a>,
    <a href="projects/multiretrieval/multiretrieval.html">project page</a>
  </li>
  <p>
  <li>
    <a href="https://arxiv.org/pdf/1910.02930.pdf">A Case Study on Combining ASR and Visual Features for Generating Instructional Video Captions</a><br>
    <b>Jack Hessel</b>, Bo Pang, Zhenhai Zhu, and Radu Soricut<br>
    CoNLL 2019<br>
    <a href="files/2019/CoNLL2019.pdf">talk slides</a>
  </li>
  <p>
  <li>
    <a href="https://www.aclweb.org/anthology/N19-1166/">
      Something's Brewing! Early Prediction of Controversy-causing Posts from Discussion Features</a><br>
    <b>Jack Hessel</b> and Lillian Lee<br>
    NAACL 2019<br>
    <a href="http://www.cs.cornell.edu/home/llee/papers/controversy.home.html">data<a>
  </li>
  <p>
  <li>
    <a href="https://www.aclweb.org/anthology/N18-1199/">Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets</a><br>
    <b>Jack Hessel</b>, David Mimno, and Lillian Lee<br>
    NAACL 2018<br>
    <a href=projects/concreteness/concreteness.html>code/data</a>
  </li>
  <p>
    <li> <a href="https://arxiv.org/abs/1703.01725">Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity</a><br>
    <b>Jack Hessel</b>, Lillian Lee, David Mimno<br>
    WWW 2017<br>
    <a href = projects/cats/cats.html>code/data</a>,
    <a href = files/2017/WWW_2017.pdf>slides</a>,
    <a href = projects/reddit/gaps.html>replication</a></li>
  <p>
    <li> <a href="https://avindhsig.wordpress.com/aligning-images-and-text-in-a-digital-library/">Aligning Images and Text in a Digital Library</a><br>
    <b>Jack Hessel</b> and David Mimno<br>
    Computer Vision in Digital Humanities Workshop at DH 2017 (extended abstract)
      <!--<a href="http://archive.is/vNUpY">backup</a>-->
    </li>
  <p>
  <li>
    <a href="https://arxiv.org/abs/1612.07487">Science, AskScience and BadScience: On the Coexistience of Highly Related Communities</a><br>
    <b>Jack Hessel</b>, Chenhao Tan, and Lillian Lee<br>
    ICWSM 2016<br>
    <a href="projects/reddit_hrc/hrc.html">data</a>,
    <a href="files/2016/ICWSM2016.pdf">slides</a>,
    <a href ="projects/reddit/gaps.html">replication</a>
  </li>
  <p>
    <li><a href="http://arxiv.org/abs/1511.03371">What do Vegans do in their Spare Time? Latent Interest Detection in Multi-Community Networks</a> <br>
    <b>Jack Hessel</b>, Alexandra Schofield, Lillian Lee, David Mimno<br>
    Workshop on Networks in the Social and Information Sciences at NeurIPS 2015<br>
    <a href="projects/latent_interest/latent_interest.html">project page</a> </li>
  <p>
  <li>
    <a href="https://www.aclweb.org/anthology/W15-2807/">Image Representations and New Domains in Neural Image Captioning</a><br>
    <b>Jack Hessel</b>, Nicolas Savva, and Kimberly J. Wilber<br>
    Workshop on Vision and Language at EMNLP 2015<br>
    <a href="files/2015/VLTalk.pdf">slides</a>
  </li>
  <p>
    <li><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.8544&rep=rep1&type=pdf">Using Reproductive Altruism to Evolve Multicellularity in Digital Organisms</a><br>
      <b>Jack Hessel</b> and Sherri Goings<br>
      ECAL 2013
    </li>
</ul>

<h3> Work Experience </h3>
<ul>
  <li> I was a postdoc and resesarch scientist on the Mosaic team at AI2 from October 2020 through October 2023.
  <li> I spent the summer of 2019 working with Google's <a href="https://research.google.com/teams/nlu/">natural language understanding</a> team.
  <li> I was a visiting faculty member at Carleton College in Spring, 2019; I taught two classes: <a href="https://jmhessel.github.io/CS322-SP19/">Natural Language Processing</a> and Discrete Math.
  <li> I spent the summer of 2018 working with Google's <a href="https://research.google.com/teams/nlu/">natural language understanding</a> team. 
  <li> I spent the summer of 2017 working with <a href="https://research.fb.com/category/data-science/">Facebook's core data science</a> team.
  <li> I spent the summer of 2016 working with the <a href="https://engineering.twitter.com/cortex">Twitter Cortex</a> team in NYC.
</ul>



<h3>Invited Talks</h3>

My CV is more up-to-date, but I've been fortunate to speak at (roughly alphabetically): Adobe Research, Carnegie Mellon University, Carleton College, Cornell, University of North Carolina (Chapel Hill),  Univeristy of Pittsburgh, Rutgers University, Seoul National University, SRI International, University of Washington, and more!

<h3>Service/Guest Lectures/Other Activites</h3>

My CV is more up to date, but I have reviewed/ACed/etc. for many NLP/CV/ML venues since 2016 including ACL, EMNLP, NAACL, AACL, EACL, AAAI, CoNLL, ACL Rolling Review, ICML, NeurIPS, ICLR, JAIR, ICWSM and more!

<h3>Other fun projects</h3>
<ul>
  <li>I wrote a <a href="https://github.com/jmhessel/recursive_nn_tf2">TreeLSTM in tensorflow 2;</a> this is a neural network whose topology changes based on each input example.</li>
  <li>I wrote a <a href="https://github.com/jmhessel/fmpytorch">factorization machine layer in pytorch;</a> for speed reasons, the forward and backward passes are written in cython.</li>
  <li>I implemented <a href="https://github.com/jmhessel/FightingWords">Monroe et al.'s "Fightin' Words" algorithm</a> for robustly comparing word frequencies in two corpora. This implementation has been used in several publications, e.g., <a href="https://www.aclweb.org/anthology/W16-0204">this</a> and <a href="https://www.aclweb.org/anthology/N19-1166/">this</a></li>
  <li>As part of an <a href="http://www.nsf.gov/crssprgm/reu/">NSF REU</a>,
    I contributed to the implementation of a really (really) fast <a href="http://en.wikipedia.org/wiki/Support_vector_machine">SVM</a> solver in <a href="http://www.cs.cornell.edu/~kilian/">Kilian Weinberger's</a> lab at <a href="http://wustl.edu/">Washington University, St. Louis.</a>  [now at Cornell!] see <a href="http://arxiv.org/abs/1404.1066"> Parallel Support Vector Machines in Practice</a> by Tyree, S., Gardner, J. R., Weinberger, K. Q., Agrawal, K., & Tran, J. (2014).</li>
  <li> <a href="http://www.micsymposium.org/mics2014/ProceedingsMICS_2014/mics2014_submission_24.pdf">"A Comparative Analysis of Popular Phylogenetic Reconstruction Algorithms."</a> Undergraduate thesis project/best paper award at <a href="http://www.micsymposium.org/mics2014/">MICS 2014</a>: joint work with Evan Albright, Nao Hiranuma, Cody Wang, and Sherri Goings.</li>
</ul>

<h3>More?</h3>

<p>I grew up in beautiful <a href="http://www.portolavalley.net/">Portola Valley, California</a>. I earned a B.A. from <a href="http://www.carleton.edu/">Carleton College</a> in 2014, studying <a href="http://cs.carleton.edu">computer science</a> and <a href="http://math.carleton.edu">mathematics/statistics</a>. During my time in <a href="http://www.ci.northfield.mn.us/">Northfield</a>, I played <a href="https://apps.carleton.edu/campus/rec/club/?item_id=81053">ice hockey,</a> and hosted a <a href="http://krlx.org/">radio show.</a> I even returned to Carleton briefly in 2019, this time, as a visiting faculty member. I'm a die hard <a href="http://sharks.nhl.com/">San Jose Sharks</a> fan, avid consumer (and very occasionally a producer) of <a href="https://soundcloud.com/jackhessel/following">electronic music</a>, and, an amateur lockpick. During graduate school at Cornell, I was a member of <a href="http://stewartlittle.org/">Stewart Little Coop,</a> a  community of 15 people, I played ice hockey in the <a href="https://iaha.sportngin.com/">Ithaca Hockey Association</a> (and, during summer internships in CA, in the <a href="http://www.solar4americaiceatsanjose.com/adult-hockey">San Jose Adult Hockey League</a>).

</p>
</body>
</html>
